---
title: "| ![](logo.png){width=6in} \\vspace{0.2in} \n`r format(params$term)` Project
  proposal\n \\vspace{0.1in} "
author:
- Maciej Szczutko
- Ewa Stebel
date: "`r Sys.time()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Uber app delivery time estimation
params:
  term: ''
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r data}
library(latex2exp)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(mgcv)
library(gam)
source("helpers.R")
library(Metrics)
UberDataset <- read.csv("UberDataset.csv")
```

\newpage

# Introduction

In our proposal we would like to describe the data we choose for Semiparametric regression final project. Course is conducted by Prof. J. Harezlak. Our goal is to explore the data and build a model which will help us answer several questions which we will propose in the latter part of this proposal. 

# Data description

```{r display-data}
set.seed(42)
kbl(slice_sample(UberDataset, n=5), booktabs = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
```

We choose [UberDataset](https://www.kaggle.com/datasets/bhanupratapbiswas/uber-data-analysis/data) from [Kaggle](https://www.kaggle.com) website. The dataset contains information about the provision of transportation services by Uber - a multinational transportation network company that operates a platform connecting riders with drivers through a mobile app. Data includes information on trips made in the USA in $2016$ year. This dataset consists of $1156$ observation of the following $7$ columns:

	1. start date - date and time of service start,
	2. end date - date and time of service end,
	3. category - categorical variable, division of the trip into private and business,
	4. start - location - city or district -  of the starting point,
	5. end - location - city or district - of the final point,
	6. miles - distance travelled in miles,
	7. purpose - categorical variable, purpose of the transport.
	



# Goal

The first thing we would like to do is standard data preprocessing to deeper understanding and find possible garbage in data (non realistic delivery time, NA values etc.). 
We want to built model (or models) to estimate delivery time.  Delivery time variable will be additional column created by transformation of start date and end date variables. For the simplest model, with one predictor, we try add some smooth terms and compare results. As the data consist observation for various location we want to analyse them by some region e.g. New York should be consider separately from Carry district. Also we would like to test relation between daytime and time of service using models with interactions. Moreover we suspect there the category of service has no significant impact on delivery time. We will use statistical tests to confirm (or reject) this hypothesis.


```{r calculate-service-time}
df <- UberDataset

### unify DATE FORMAT ###
df$START_DATE <- gsub("/", "-", df$START_DATE)
df$END_DATE <- gsub("/", "-", df$END_DATE)

df$START_DATE <- as.POSIXct(df$START_DATE, format = "%m-%d-%Y %H:%M")
df$END_DATE <- as.POSIXct(df$END_DATE, format = "%m-%d-%Y %H:%M")
df$SERVICE_TIME <- as.numeric(df$END_DATE - df$START_DATE) / 60
```


## Preliminary analysis

TODO SECTION: Add some basic plots e.g. location popularity chart etc.

In order to gain a more thorough understanding of the data analyzed and to select models more effectively, we conducted a preliminary analysis. First, we created a graph below of the relationship between the miles variable and service time by initial destination.

Analyzing the locations from the start and stop variables, we see that the frequency of occurrence of a given location is various. Therefore, we selected the 15 most frequently occurring locations and presented their frequency in a histogram. In our analyzed dataset, we also filtered out the most frequent locations. 

```{r start-location-hist}

t1 = table(factor(df$START))
t2 = table(factor(df$STOP))
# sort(t2, decreasing = TRUE)

top15_column1 <- head(sort(t1, decreasing = TRUE), 15)
top15_column2 <- head(sort(t2, decreasing = TRUE), 15)

top15 = top15_column1 + top15_column2

df_top15 <- data.frame(Wartość = names(top15), Liczba_Wystąpień = as.numeric(top15))

ggplot(df_top15, aes(x = Wartość, y = Liczba_Wystąpień)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Histogram of locations", x = "Location", y = "Number of appearances")

```

From the chart, we can see that some locations form a distinct group on the graph. For example, service which has starting point in Raleight (violet points) is concentrated mostly above line with service time equal to 20 minutes. This suggests taking this variable into account when building the model. In addition, service which has starting point in Durham concentrates around the straight miles equals to 10, making us think that the variable miles and starting location may be correlated.

```{r filter-local-data}
filtered_destination <- df %>% filter(START %in% c("Cary", "Morrisville", "Apex", "Raleigh", "Durham") | STOP %in% c("Cary", "Morrisville", "Apex", "Raleigh", "Durham")) %>% filter(MILES < 20)

```
We see that the Cary location occurs much more frequently than the others. The Raleight, Durcham, Morrisville and Apex locations appear frequently, while the other locations appear sporadically.


```{r simple-plot}

p <- ggplot(filtered_destination, aes(x=MILES, y=SERVICE_TIME, colour = factor(START)))
p + geom_point()

```
We also see that some locations occur more frequently than others, so we also created a histogram of the popularity of a location.

```{r start-location-hist2}
ggplot(filtered_destination, aes(x = START)) +
  geom_bar(fill = "blue", color = "black", alpha = 0.7)
```
We see that the Cary location occurs much more frequently than the others. The Raleight, Durcham, Morrisville and Apex locations appear frequently, while the other locations appear sporadically.

# Data cleaning  

To prepare the data for modelling, we must first clean it. To do this, we replace the NA values in the Purpose column with "Unknown". Other columns do not show missing values, however the start and stop columns show some "Unknown location" values. We also remove the last row of data, which is the summary.

We added two columns to the raw data - Service time, which shows the duration of the service in minutes. To do this, we first standardized the format of the start and end date. Second added column is Daytime column, which indicates whether the service took place in rush hours (15-18) or regular hours (all others). We created this division based on the boxplot below. This division is also in line with our intuition. We believe this variable can be significant for modeling. The short summary of data after cleansing is presented below.



```{r data-preprocessing}
#CLEANSING
df <- filtered_destination

df$PURPOSE[which(df$PURPOSE=="")] = "Unknown"
df = df[-dim(df)[1],] #deleting summary

# DAYTIME
library(lubridate)
hours = hour(ymd_hms(df$END_DATE))

any(is.na(hours)) ## FALSE
hours[which(is.na(hours))]=-1 # seems to be redundant

for (i in 1:length(df$END_DATE)){
  if ( 15 <= hours[i]  &  hours[i] <= 18){
    df$DAY_TIME[i]="2"
  }else {
    df$DAY_TIME[i] = "1"
  }
  
}

summary(df)
```

```{r plot-fill-by-hours}

df$HOUR <- hours

p <- ggplot(df, aes(x=MILES, y=SERVICE_TIME, colour = factor(DAY_TIME)))
p + geom_point()

```

```{r boxplot-by-hours}
p <- ggplot(df, aes(x=factor(HOUR), y=SERVICE_TIME))
p + geom_boxplot()

```
Based on the boxplots, we see variation in service time by hour of the day, suggesting the importance of this variable


```{r boxplot-by-purpose-category}

par(mfrow=c(2,1))
p <- ggplot(df, aes(x=factor(CATEGORY), y=SERVICE_TIME))
p + geom_boxplot()

p <- ggplot(df, aes(x=factor(PURPOSE), y=SERVICE_TIME))
p + geom_boxplot()

```
We do not see a significant difference in service time due to the breakdown by travel category.  The average
time and distance between quantiles for personal trips are slightly smaller than for buisness trips.

In the case of the purpose variable, we see differences between factors, so we will also consider this variable in later modeling. 

In order to better understand the data, we have put the locations from the dataset onto a map of the United States. On the map, we can see that the locations are divided into a pair of clusters, between which the average delivery time may also vary.

Once the data has been cleaned and subjected to preliminary analysis, we can begin to build models that predict service times. The first basic model we will create is a linear regression model with an explanatory variable - service time and an explanatory variable - miles. The linear regression model has the form 
$$ f(X) = \beta_0 + \sum_{j=1}^{p} x_j \beta_j, $$
where the $\beta_j$ ' are unknown parameters or coefficients.

This is a basic method, but allows us an easy interpretation of regressors effects.


```{r simple-reression-model}
simlest_model <- lm(data = df, SERVICE_TIME~MILES)

summary(simlest_model)

p <- ggplot(df, aes(x=MILES, y=SERVICE_TIME))
p + geom_point(colour="darkred")+
  geom_abline(intercept = coefficients(simlest_model)[1], slope = coefficients(simlest_model)[2])

```
The regression line captures the general trend of the data well, but we can see that a straight line is not the best method of prediction in this case. 

Next, we use gam function from **mgcv** package to create Generalized Additive Model. It is a Generalized Linear Model (GLM) in which the linear predictor is given by a specified sum of smooth functions of the covariates plus a conventional parametric component of the linear predictor.We can define with regression splinesusing an equation
$$ f(x) = \beta_0 + \beta_1+x + \sum_{k=1}^K b_k(x - x_k) $$

First, we constructed model using only miles as explanatory variable. In our second model we combined miles and hours as explanatory variables. The best model for these two variables is the model where we used the miles ad smoothing term. In this model we also testes the influence of different number of knots. As we know selection of number of knots i crucial, so we compared models with 2,10 and 30 number of knots. As we see first model is slightly different from the linear model, second seems to adjusts to the punctures correctly. Hovewer last model may present overfitting.


```{r model-with-hours}
##ewa
gam_model_hours <- gam(df$SERVICE_TIME ~ s(df$MILES) + df$HOUR)
summary(gam_model_hours)

```




```{r gam-simplest-model}
y <- df$SERVICE_TIME
x <- df$MILES

xg <- seq(min(x), max(x), length = 1001)

gam_model <- gam(y ~ s(x,2))
gam_model1 = gam(y ~ s(x,10))
gam_model2 = gam(y ~ s(x,30))

linear_model_using_gam <- gam(y~x)

AIC(gam_model2)

predicted <- predict(gam_model, newdata = data.frame(x = xg))
predicted1 <- predict(gam_model1, newdata = data.frame(x = xg))
predicted2 <- predict(gam_model2, newdata = data.frame(x = xg))

orginal_points = data.frame(Y = y, X=x)
predicted_points = data.frame(Y = predicted, X=xg)
predicted_points1 = data.frame(Y = predicted1, X=xg)
predicted_points2 = data.frame(Y = predicted2, X=xg)


ggplot() +
  geom_point(data = orginal_points , aes(x=X, y=Y), colour="black")+
  geom_line(data = predicted_points, aes(x=X, y=Y), colour = "darkblue")+
  geom_line(data = predicted_points1, aes(x=X, y=Y), colour = "red")+
  geom_line(data = predicted_points2, aes(x=X, y=Y), colour = "darkgreen")
 
```


## Linearity test

```{r test-for-linearity}

anova(linear_model_using_gam, gam_model, test="F")$"Pr(>F)"[2]

## maciek opisze co jest uzywane w testowaniu i jakie som wnioski

```

## Gam with interactions beetwen day time and distance. 

```{r gam-with-interactions-model}
y <- df$SERVICE_TIME
x <- df$MILES
interaction_term <- df$DAY_TIME

gam_model_inter <- mgcv::gam(y ~ s(x, by = factor(interaction_term)))

summary(gam_model)
```

```{r gam-with-interactions-model-plots}
xg <- seq(min(x), max(x), length = 1001)
day_time_1  <- rep(1, length = 1001)
day_time_2  <- rep(2, length = 1001)

predicted_1 <- predict(gam_model, newdata = data.frame(x = xg, interaction_term = day_time_1))
predicted_2 <- predict(gam_model, newdata = data.frame(x = xg, interaction_term = day_time_2))

orginal_points = data.frame(Y = y, X=x)
predicted_points = data.frame(Y1 = predicted_1, Y2=predicted_2, X=xg)

ggplot() +
  geom_point(data = orginal_points , aes(x=X, y=Y), colour="darkred")+
  geom_line(data = predicted_points, aes(x=X, y=Y1), colour = "darkblue")+
  geom_line(data = predicted_points, aes(x=X, y=Y2), colour = "darkgreen")

```


## Test framework to automatic feature selection 
<!-- section created by Ewa -->

To see if the other variables in the data set affect service time we used the function step. Gam from the gam library. This method creates all possible models based on possible variables and calculates the **AIC** for each model. The result is a model that uses the variables hour, start, stop and miles as smoothing term.

```{r step-gam-all-variables}
fitInitial = gam(y ~ df$MILES + df$HOUR + df$CATEGORY+df$START +df$STOP +df$PURPOSE)

stepFit <- step.Gam(fitInitial,
                    scope = list("HOUR" = ~ 1 + df$HOUR + s(df$HOUR,2),
                                 "MILES" = ~ 1 + df$MILES + s(df$MILES,2),
                                 "CATEGORY" = ~ 1 + df$CATEGORY,
                                 "START" = ~ 1 + df$START,
                                 "STOP" = ~ 1 + df$STOP,
                                 "PURPOSE" = ~ 1 + df$PURPOSE))
print(names(stepFit$"model")[-1])

step_gam_model <- gam(y~ df$HOUR + s(df$MILES, 2) + df$START+ df$STOP)
summary(step_gam_model)
```


## Metric evaluation for different models

We use k-folds cross-validation to evaluate RMSE, MAE and $R^2$. In this approach, we split previously filtered data set, into $k$ disjoint subset with (almost) equal size. The model is then trained $k$ times, each time using $k-1$ folds as the training set and the remaining fold as the test set. This ensures that every data point is used for testing exactly once. For ordinary linear model we have ready to use solution, but for gam model we need to implement such functionality (see helpers.R for implementation). We compare $4$ models:

Model name | Model formula
------------- | -------------
LM | SERVICE_TIME ~ MILES
LM with interactions | SERVICE_TIME ~ MILES * DAY_TIME
GAM | SERVICE_TIME ~ s(MILES)
GAM with interactions | SERVICE_TIME ~ s(MILES * DAY_TIME)

### Metrics definitons 

$$
MAE = \frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}_i\right|
$$


$$
RMSE =  \sqrt{\frac{1}{n}\sum_{i=1}^n\left (  y_i-\hat{y}_i\right)^2}
$$
$$
\mathrm{R}^2=1-\frac{\text { Unexplained Variation }}{\text { Total Variation }}
$$

It's not official $R^2$ formula, rather intuition that we should have during evaluation of model performance. First two metrics inform us about predictive power of our model (but in slightly different way). We want to minimize them both. $R^2$ tell how many percent of variance in data we explain (higher=better), but it shouldn't be use as base criterium for model comparison with different numbers of features (always increase when number of features increase), but still can be useful. 

Here we use $k=5$ folds. Models are evaluated on same folds.

```{r cv-for-gam-models}
k <- 5

set.seed(42) # use seed to reproduce results
resulting_folds <- split_data_into_folds(df, k)
dfWithMetrics <- data.frame(RMSE = c(), R_squared = c(), fold_number = c(), model_type = c())


for(i in 1:k)
{
  test_set <- resulting_folds[[i]]
  
  
  train_data <- resulting_folds[ seq(k)[-i]]
  train_data <- do.call(rbind, train_data)

  # simple lm model
  simlest_model_for_folds <- lm(data = train_data, SERVICE_TIME~MILES)
  predicted <- predict(simlest_model_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(simlest_model_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "LM")) 
  
  ## simple lm but with iteraction
  lm_with_interaction_for_folds <- lm(data = train_data, SERVICE_TIME~MILES*DAY_TIME)
  predicted <- predict(lm_with_interaction_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(lm_with_interaction_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "LM with interaction")) 
  
  ## gam model
  gam_model_for_folds <- mgcv::gam(data = train_data, SERVICE_TIME ~ s(MILES))
  predicted <- predict(gam_model_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(gam_model_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "GAM"))  
  
  ## gam model with interaction
  
  gam_model_inter_for_folds <- mgcv::gam(data = train_data, SERVICE_TIME ~ s(MILES, by = factor(DAY_TIME)))
  predicted <- predict(gam_model_inter_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(gam_model_inter_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "GAM with interaction"))
}
```

```{r fix-data-types}
## [minor]ToDo: why we end with numeric for this columns? It's unexpected...
colnames(dfWithMetrics) <- c("MAE","RMSE", "R_squared", "Fold_number", "Model")
dfWithMetrics$MAE <- as.numeric(dfWithMetrics$MAE)
dfWithMetrics$RMSE <- as.numeric(dfWithMetrics$RMSE)
dfWithMetrics$R_squared <- as.numeric(dfWithMetrics$R_squared)
```


```{r plot-metrics}
RMSE_plot <- ggplot(data = dfWithMetrics, aes(x = Fold_number, y = RMSE, color = Model, shape = Model, group = Model))+
  ggtitle("RMSE per fold")+
  geom_point()+
  geom_line(aes(y = ave(RMSE, Model, FUN = mean, linetype="dashed")))

MAE_plot <- ggplot(data = dfWithMetrics, aes(x = Fold_number, y = MAE, color = Model, group = Model, shape = Model))+
  ggtitle(TeX("MAE per fold"))+
  geom_point()+
  geom_line(aes(y = ave(MAE, Model, FUN = mean, linetype="dashed")))

ggarrange(RMSE_plot, MAE_plot, common.legend = TRUE, legend = "bottom", ncol = 2, nrow=1)
```

On plots we see the GAM has lower RMSE and MAE on average. Adding interaction to GAM seems to be pointless. They contribute nothing to the model. I suspect the number of parameters might be a bit too high or the way we developed *DAY_TIME* predictor it's not optimal. By contrast, we observe the ordinary LM model gain if we add interaction (despite increasing numbers of parameters). So maybe the idea was not so bad.


From the $R^2$ perspective the both GAM models has significantly better score. However, the number of parameters used in models is notably greater. In fact they ordered by number of parameters. That's only reassures me to not use $R^2$ as reference here.


```{r plots-R-squared}
R_squared_plot <- ggplot(data = dfWithMetrics, aes(x = Fold_number, y = R_squared, color = Model, group = Model, shape = Model))+
  ggtitle(TeX("$R^2$ per fold"))+
  geom_point()+
  geom_line(aes(y = ave(R_squared, Model, FUN = mean, linetype="dashed")))
R_squared_plot
```


To sum up, we would leave idea with adding interaction term to the model as it seems to be redundant and choose non linear one as best.

