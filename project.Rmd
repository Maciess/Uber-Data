---
title: "| ![](logo.png){width=6in} \\vspace{0.2in} \n`r format(params$term)` Project
  proposal\n \\vspace{0.1in} "
author:
- Maciej Szczutko
- Ewa Stebel
date: "`r Sys.time()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Uber app delivery time estimation
params:
  term: ''
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align="center", fig.align='center', out.width='70%')
```

```{r data}
library(latex2exp)
library(kableExtra)
library(knitr)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(mgcv)
library(gam)
source("helpers.R")
library(Metrics)
library(magick)
UberDataset <- read.csv("UberDataset.csv")
```

\newpage

# Introduction

In our proposal we would like to describe the data we choose for Semiparametric regression final project. Course is conducted by Prof. J. Harezlak. Our goal is to explore the data and build a model which will help us answer several questions which we will propose in the latter part of this proposal. 

# Data description

```{r display-data}
set.seed(42)
kbl(slice_sample(UberDataset, n=5), booktabs = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
```

We choose [UberDataset](https://www.kaggle.com/datasets/bhanupratapbiswas/uber-data-analysis/data) from [Kaggle](https://www.kaggle.com) website. The dataset contains information about the provision of transportation services by Uber - a multinational transportation network company that operates a platform connecting riders with drivers through a mobile app. Data includes information on trips made in the USA in $2016$ year. This dataset consists of $1156$ observation of the following $7$ columns:

	1. start date - date and time of service start,
	2. end date - date and time of service end,
	3. category - categorical variable, division of the trip into private and business,
	4. start - location - city or district -  of the starting point,
	5. end - location - city or district - of the final point,
	6. miles - distance travelled in miles,
	7. purpose - categorical variable, purpose of the transport.
	



# Goal

The first thing we would like to do is standard data preprocessing to deeper understanding and find possible garbage in data (non realistic delivery time, NA values etc.). 
We want to built model (or models) to estimate delivery time.  Delivery time variable will be additional column created by transformation of start date and end date variables. For the simplest model, with one predictor, we try add some smooth terms and compare results. As the data consist observation for various location we want to analyse them by some region e.g. New York should be consider separately from Carry district. Also we would like to test relation between daytime and time of service using models with interactions. Moreover we suspect there the category of service has no significant impact on delivery time. We will use statistical tests to confirm (or reject) this hypothesis.


```{r calculate-service-time}
df <- UberDataset

### unify DATE FORMAT ###
df$START_DATE <- gsub("/", "-", df$START_DATE)
df$END_DATE <- gsub("/", "-", df$END_DATE)

df$START_DATE <- as.POSIXct(df$START_DATE, format = "%m-%d-%Y %H:%M")
df$END_DATE <- as.POSIXct(df$END_DATE, format = "%m-%d-%Y %H:%M")
df$SERVICE_TIME <- as.numeric(df$END_DATE - df$START_DATE) / 60
```


## Preliminary analysis

```{r all-location-on-map, fi.cap='The map with all posible location present in UberDataset', fig.align='center', out.width='70%'}
knitr::include_graphics("location_distribution.png")
```

In order to better understand the data, we have put the locations from the dataset onto a map of the United States. On the map, we can see that the locations are divided into a pair of clusters, between which the average delivery time may also vary.

```{r, out.width='80%', fig.align='center'}

path <- "mapa_screen.png"
photo <- image_read(path)

knitr::include_graphics(path)
```

In order to gain a more thorough understanding of the data analyzed and to select models more effectively, we conducted a preliminary analysis. First, we created a graph below of the relationship between the miles variable and service time by initial destination.

Analyzing the locations from the start and stop variables, we see that the frequency of occurrence of a given location is various. Therefore, we selected the 15 most frequently occurring locations and presented their frequency in a histogram. In our analyzed dataset, we also filtered out the most frequent locations. 

```{r start-location-hist}

t1 = table(factor(df$START))
t2 = table(factor(df$STOP))
# sort(t2, decreasing = TRUE)

top15_column1 <- head(sort(t1, decreasing = TRUE), 15)
top15_column2 <- head(sort(t2, decreasing = TRUE), 15)

top15 = top15_column1 + top15_column2

df_top15 <- data.frame(Wartość = names(top15), Liczba_Wystąpień = as.numeric(top15))
df_top15$Wartość[which(df_top15$Wartość=="Kar?chi")] = "Karachi"

ggplot(df_top15, aes(x = Wartość, y = Liczba_Wystąpień)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Histogram of locations", x = "Location", y = "Number of appearances")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
We see that the Cary location occurs much more frequently than the others. The Raleight, Durcham, Morrisville and Apex locations appear frequently, while the other locations appear sporadically.



```{r filter-local-data}
filtered_destination <- df %>% filter(START %in% c("Cary", "Morrisville", "Apex", "Raleigh", "Durham") | STOP %in% c("Cary", "Morrisville", "Apex", "Raleigh", "Durham")) %>% filter(MILES < 20)

```



```{r simple-plot}

p <- ggplot(filtered_destination, aes(x=MILES, y=SERVICE_TIME, colour = factor(START)))
p + geom_point()

```
From the chart, we can see that some locations form a distinct group on the graph. For example, service which has starting point in Raleight (violet points) is concentrated mostly above line with service time equal to 20 minutes. This suggests taking this variable into account when building the model. In addition, service which has starting point in Durham concentrates around the straight miles equals to 10, making us think that the variable miles and starting location may be correlated.



# Data cleaning  

To prepare the data for modelling, we must first clean it. To do this, we replace the NA values in the Purpose column with "Unknown". Other columns do not show missing values, however the start and stop columns show some "Unknown location" values. We also remove the last row of data, which is the summary.

We added two columns to the raw data - Service time, which shows the duration of the service in minutes. To do this, we first standardized the format of the start and end date. Second added column is Daytime column, which indicates whether the service took place in rush hours (15-18) or regular hours (all others). We created this division based on the boxplot below. This division is also in line with our intuition. We believe this variable can be significant for modeling. The short summary of data after cleansing is presented below.



```{r data-preprocessing}
#CLEANSING
df <- filtered_destination

df$PURPOSE[which(df$PURPOSE=="")] = "Unknown"
df = df[-dim(df)[1],] #deleting summary

# DAYTIME
library(lubridate)
hours = hour(ymd_hms(df$END_DATE))

any(is.na(hours)) ## FALSE
hours[which(is.na(hours))]=-1 # seems to be redundant

for (i in 1:length(df$END_DATE)){
  if ( 15 <= hours[i]  &  hours[i] <= 18){
    df$DAY_TIME[i]="2"
  }else {
    df$DAY_TIME[i] = "1"
  }
  
}

summary(df)
```
We also analyzed a newly created variable - hours and day time.
```{r plot-fill-by-hours}

df$HOUR <- hours

p <- ggplot(df, aes(x=MILES, y=SERVICE_TIME, colour = factor(DAY_TIME)))
p + geom_point()

```

```{r boxplot-by-hours}
p <- ggplot(df, aes(x=factor(HOUR), y=SERVICE_TIME))
p + geom_boxplot()

```
We are not able to see a clear division by time of day on the scatterplot, however based on the boxplot, we see variation in service time by hour of the day, which suggest the importance of this variable.

Then, we checked the other two variables included in the dataset - purpose and category and their impact on service time.

```{r boxplot-by-purpose-category}

par(mfrow=c(2,1))
p <- ggplot(df, aes(x=factor(CATEGORY), y=SERVICE_TIME))
p + geom_boxplot()

p <- ggplot(df, aes(x=factor(PURPOSE), y=SERVICE_TIME))
p + geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
We do not see a significant difference in service time due to the breakdown by travel category.  The average
time and distance between quantiles for personal trips are slightly smaller than for buisness trips.

In the case of the purpose variable, we see differences between factors, so we will also consider this variable in later modeling. 



## Models

Once the data has been cleaned and subjected to preliminary analysis, we can begin to build models that predict service times. The first basic model we will create is a linear regression model with an explained variable - service time and an explanatory variable - miles. The linear regression model has the form 
$$ f(X) = \beta_0 + \sum_{j=1}^{p} x_j \beta_j, $$
where the $\beta_j$ ' are unknown parameters or coefficients.

This is a basic method, but allows us an easy interpretation of regressors effects.


## Simple linear model

```{r simple-reression-model}
simlest_model <- lm(data = df, SERVICE_TIME~MILES)

summary(simlest_model)
```

```{r plot-for-first-model, fig.cap="Ordinary linear model", out.width="70%"}
p <- ggplot(df, aes(x=MILES, y=SERVICE_TIME))
p + geom_point(colour="darkred")+
  geom_abline(intercept = coefficients(simlest_model)[1], slope = coefficients(simlest_model)[2])
```

The regression line captures the general trend of the data well, but we can see that a straight line is not the best method of prediction in this case. 

## Generalized Additive Model

Next, we use gam function from **mgcv** package to create Generalized Additive Model. It is a Generalized Linear Model (GLM) in which the linear predictor is given by a specified sum of smooth functions of the covariates plus a conventional parametric component of the linear predictor.We can define with regression splines using an equation
$$ f(x) = \beta_0 + \beta_1+x + \sum_{k=1}^K b_k(x - x_k) $$

<<<<<<< HEAD
We would like to compare a simple linear regression model and a model with an additional hours variable and applied smoothing on the miles variable. AIC value of simple model is equal to 2 283.44, while AIC value of second model equals 2 269.84. With this comparison, we can conclude that the use of splines and day_time variable made the model better fit the data.
=======
First, we constructed model using only miles as explanatory variable. In our second model we combined miles and hours as explanatory variables. The best model for these two variables is the model where we used the miles and smoothing term. In this model we also testes the influence of different number of knots. As we know selection of number of knots is crucial, so we compared models with 2, 10 and 30 number of knots. As we see first model is slightly different from the linear model, second seems to adjusts to the punctures correctly. However last model may present overfitting. The conclusion is to delegate the decision about knots number for model as we don't have reason to do it manually.

>>>>>>> ad655d2 (Add plot legends)

```{r model-with-hours}
##ewa
gam_model_hours <- gam(df$SERVICE_TIME ~ s(df$MILES) + df$DAY_TIME)
summary(gam_model_hours)

```

Then, we would like to test the influence of different number of knots in model with only miles as smoothing variable.  As we know selection of number of knots i crucial, so we compared models with 2, 10 and 30 number of knots. As we see first model is slightly different from the linear model,so, as we noted earlier, the model only captures the general trend. Second model seems to adjusts to the punctures correctly. Hovewer last model may present overfitting, because is sensitive to the impact of single observations.







```{r gam-simplest-model}
y <- df$SERVICE_TIME
x <- df$MILES

xg <- seq(min(x), max(x), length = 1001)

gam_model <- gam(y ~ s(x,2))
gam_model1 = gam(y ~ s(x,10))
gam_model2 = gam(y ~ s(x,30))

predicted <- predict(gam_model, newdata = data.frame(x = xg))
predicted1 <- predict(gam_model1, newdata = data.frame(x = xg))
predicted2 <- predict(gam_model2, newdata = data.frame(x = xg))

orginal_points = data.frame(Y = y, X=x)
predicted_points = data.frame(Y = predicted, X=xg)
predicted_points1 = data.frame(Y = predicted1, X=xg)
predicted_points2 = data.frame(Y = predicted2, X=xg)


ggplot() +
  geom_point(data = orginal_points , aes(x=X, y=Y), colour="darkred", size = 1)+
  geom_line(data = predicted_points, aes(x=X, y=Y, colour = "k=2"))+
  geom_line(data = predicted_points1, aes(x=X, y=Y, colour = "k=10"))+
  geom_line(data = predicted_points2, aes(x=X, y=Y, colour = "k=30"))+
 scale_colour_manual("", 
                      breaks = c("k=2", "k=10", "k=30"),
                      values = c("darkblue", "purple", "darkgreen"))+
  ggtitle("Models with differents number of knots")+
  xlab("MILES")+
  ylab("TIME")
 
```


## Linearity test

Now we will perform test for linearity of $f$ function. Recall:
We assume that relation is in the form

$$
y_i=f\left(x_i\right)+\varepsilon_i, \quad \varepsilon_i \stackrel{\text { ind. }}{\sim} N\left(0, \sigma_{\varepsilon}^2\right)
$$

and we consider following hypothesis

$$H_0: f \text{ is linear versus }H_1: f\text{ is a smooth non-linear function.}$$

We will use $F$ test. 

```{r test-for-linearity, echo=TRUE}
linear_model_using_gam <- gam(y~x)
anova(linear_model_using_gam, gam_model, test="F")$"Pr(>F)"[2]
```
Based on p value, using standard $\alpha=0.05$ confidence level, we reject $H_0$. The relation between SERVICE_TIME and MILES isn't strictly linear and using smoothing term should lead us to better model. In next section we will try to add interaction with previously developed feature - DAY_TIME.


## Gam with interactions beetwen day time and distance. 

```{r gam-with-interactions-model}
y <- df$SERVICE_TIME
x <- df$MILES
interaction_term <- df$DAY_TIME

gam_model_inter <- mgcv::gam(y ~ s(x, by = factor(interaction_term)))

summary(gam_model)
```

```{r gam-with-interactions-model-plots}
xg <- seq(min(x), max(x), length = 1001)
day_time_1  <- rep(1, length = 1001)
day_time_2  <- rep(2, length = 1001)

predicted_1 <- predict(gam_model, newdata = data.frame(x = xg, interaction_term = day_time_1))
predicted_2 <- predict(gam_model, newdata = data.frame(x = xg, interaction_term = day_time_2))

orginal_points = data.frame(Y = y, X=x)
predicted_points = data.frame(Y1 = predicted_1, Y2=predicted_2, X=xg)

ggplot() +
  geom_point(data = orginal_points , aes(x=X, y=Y), colour="darkred")+
  geom_line(data = predicted_points, aes(x=X, y=Y1), colour = "darkblue")+
  geom_line(data = predicted_points, aes(x=X, y=Y2), colour = "darkgreen")

```


## Test framework to automatic feature selection 
<!-- section created by Ewa -->

To see if the other variables in the data set affect service time we used the function step. Gam from the gam library. This method creates all possible models based on possible variables and calculates the **AIC** for each model. The result is a model that uses the variables hour, start, stop and miles as smoothing term.

```{r step-gam-all-variables}
fitInitial = gam(y ~ df$MILES + df$HOUR + df$CATEGORY+df$START +df$STOP +df$PURPOSE)

stepFit <- step.Gam(fitInitial,
                    scope = list("HOUR" = ~ 1 + df$HOUR + s(df$HOUR,2),
                                 "MILES" = ~ 1 + df$MILES + s(df$MILES,2),
                                 "CATEGORY" = ~ 1 + df$CATEGORY,
                                 "START" = ~ 1 + df$START,
                                 "STOP" = ~ 1 + df$STOP,
                                 "PURPOSE" = ~ 1 + df$PURPOSE))
print(names(stepFit$"model")[-1])

step_gam_model <- gam(y~ df$HOUR + s(df$MILES, 2) + df$START+ df$STOP)
summary(step_gam_model)
```


## Metric evaluation for different models

We use k-folds cross-validation to evaluate RMSE, MAE and $R^2$. In this approach, we split previously filtered data set, into $k$ disjoint subset with (almost) equal size. The model is then trained $k$ times, each time using $k-1$ folds as the training set and the remaining fold as the test set. This ensures that every data point is used for testing exactly once. For ordinary linear model we have ready to use solution, but for gam model we need to implement such functionality (see helpers.R for implementation). We compare $4$ models:

Model name | Model formula
------------- | -------------
LM | SERVICE_TIME ~ MILES
LM with interactions | SERVICE_TIME ~ MILES * DAY_TIME
GAM | SERVICE_TIME ~ s(MILES)
GAM with interactions | SERVICE_TIME ~ s(MILES * DAY_TIME)

### Metrics definitons 

$$
MAE = \frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}_i\right|
$$


$$
RMSE =  \sqrt{\frac{1}{n}\sum_{i=1}^n\left (  y_i-\hat{y}_i\right)^2}
$$
$$
\mathrm{R}^2=1-\frac{\text { Unexplained Variation }}{\text { Total Variation }}
$$

It's not official $R^2$ formula, rather intuition that we should have during evaluation of model performance. First two metrics inform us about predictive power of our model (but in slightly different way). We want to minimize them both. $R^2$ tell how many percent of variance in data we explain (higher=better), but it shouldn't be use as base criterium for model comparison with different numbers of features (always increase when number of features increase), but still can be useful. 

Here we use $k=5$ folds. Models are evaluated on same folds.

```{r cv-for-gam-models}
k <- 5

set.seed(42) # use seed to reproduce results
resulting_folds <- split_data_into_folds(df, k)
dfWithMetrics <- data.frame(RMSE = c(), R_squared = c(), fold_number = c(), model_type = c())


for(i in 1:k)
{
  test_set <- resulting_folds[[i]]
  
  
  train_data <- resulting_folds[ seq(k)[-i]]
  train_data <- do.call(rbind, train_data)

  # simple lm model
  simlest_model_for_folds <- lm(data = train_data, SERVICE_TIME~MILES)
  predicted <- predict(simlest_model_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(simlest_model_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "LM")) 
  
  ## simple lm but with iteraction
  lm_with_interaction_for_folds <- lm(data = train_data, SERVICE_TIME~MILES*DAY_TIME)
  predicted <- predict(lm_with_interaction_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(lm_with_interaction_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "LM with interaction")) 
  
  ## gam model
  gam_model_for_folds <- mgcv::gam(data = train_data, SERVICE_TIME ~ s(MILES))
  predicted <- predict(gam_model_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(gam_model_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "GAM"))  
  
  ## gam model with interaction
  
  gam_model_inter_for_folds <- mgcv::gam(data = train_data, SERVICE_TIME ~ s(MILES, by = factor(DAY_TIME)))
  predicted <- predict(gam_model_inter_for_folds, newdata = test_set)
  
  MAE <- Metrics::mae(actual = test_set$SERVICE_TIME, predicted = predicted)
  RMSE <- Metrics::rmse(actual = test_set$SERVICE_TIME, predicted = predicted)
  R_squared <- summary(gam_model_inter_for_folds)$r.sq
  dfWithMetrics <- rbind(dfWithMetrics, c(MAE, RMSE, R_squared, i, "GAM with interaction"))
}
```

```{r fix-data-types}
## [minor]ToDo: why we end with numeric for this columns? It's unexpected...
colnames(dfWithMetrics) <- c("MAE","RMSE", "R_squared", "Fold_number", "Model")
dfWithMetrics$MAE <- as.numeric(dfWithMetrics$MAE)
dfWithMetrics$RMSE <- as.numeric(dfWithMetrics$RMSE)
dfWithMetrics$R_squared <- as.numeric(dfWithMetrics$R_squared)
```


```{r plot-metrics}
RMSE_plot <- ggplot(data = dfWithMetrics, aes(x = Fold_number, y = RMSE, color = Model, shape = Model, group = Model))+
  ggtitle("RMSE per fold")+
  geom_point()+
  geom_line(aes(y = ave(RMSE, Model, FUN = mean, linetype="dashed")))

MAE_plot <- ggplot(data = dfWithMetrics, aes(x = Fold_number, y = MAE, color = Model, group = Model, shape = Model))+
  ggtitle(TeX("MAE per fold"))+
  geom_point()+
  geom_line(aes(y = ave(MAE, Model, FUN = mean, linetype="dashed")))

ggarrange(RMSE_plot, MAE_plot, common.legend = TRUE, legend = "bottom", ncol = 2, nrow=1)
```

On plots we see the GAM has lower RMSE and MAE on average. Adding interaction to GAM seems to be pointless. They contribute nothing to the model. I suspect the number of parameters might be a bit too high or the way we developed *DAY_TIME* predictor it's not optimal. By contrast, we observe the ordinary LM model gain if we add interaction (despite increasing numbers of parameters). So maybe the idea was not so bad.


From the $R^2$ perspective the both GAM models has significantly better score. However, the number of parameters used in models is notably greater. In fact they ordered by number of parameters. That's only reassures me to not use $R^2$ as reference here.


```{r plots-R-squared}
R_squared_plot <- ggplot(data = dfWithMetrics, aes(x = Fold_number, y = R_squared, color = Model, group = Model, shape = Model))+
  ggtitle(TeX("$R^2$ per fold"))+
  geom_point()+
  geom_line(aes(y = ave(R_squared, Model, FUN = mean, linetype="dashed")))
R_squared_plot
```


To sum up, we would leave idea with adding interaction term to the model as it seems to be redundant and choose non linear one as best.

